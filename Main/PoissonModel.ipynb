{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# 1. Correct File Loading\n",
    "medal_counts = pd.read_csv('/Users/chris/MCM_2025_C/Data/summerOly_medal_counts_with_codes.csv')\n",
    "programs = pd.read_csv('/Users/chris/MCM_2025_C/Data/summerOly_programs.csv')\n",
    "hosts = pd.read_csv('/Users/chris/MCM_2025_C/Data/summerOly_hosts_with_codes.csv')  # Corrected\n",
    "\n",
    "# 2. Load Athlete Counts Data\n",
    "pattern = os.path.join('/Users/chris/MCM_2025_C/Data/athlete_probabilities_by_year', '*.csv')\n",
    "athlete_files = glob.glob(pattern, recursive=True)\n",
    "athlete_dfs = []\n",
    "\n",
    "for file in athlete_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        # Standardize column names\n",
    "        df.rename(columns={\n",
    "            'bronze': 'Bronze',\n",
    "            'silver': 'Silver',\n",
    "            'gold': 'Gold',\n",
    "            'total_athletes': 'Total_Athletes',\n",
    "            'year': 'Year'\n",
    "        }, inplace=True)\n",
    "        # Extract Year from filename if 'Year' column is absent\n",
    "        if 'Year' not in df.columns:\n",
    "            year_str = os.path.splitext(os.path.basename(file))[0].split('_')[-1]\n",
    "            df['Year'] = int(year_str)\n",
    "        athlete_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Aggregate the athlete_probabilities_by_year DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate all athlete DataFrames\n",
    "athlete_counts = pd.concat(athlete_dfs, ignore_index=True)\n",
    "\n",
    "# 3. Aggregate Athlete Counts\n",
    "if 'Total_Athletes' in athlete_counts.columns:\n",
    "    athlete_counts_agg = athlete_counts.groupby(['Year', 'Country Code']).agg({\n",
    "        'Total_Athletes': 'sum'\n",
    "    }).reset_index()\n",
    "else:\n",
    "    # If 'Total_Athletes' is not present, count the number of athletes\n",
    "    athlete_counts_agg = athlete_counts.groupby(['Year', 'Country Code']).size().reset_index(name='Total_Athletes')\n",
    "\n",
    "\n",
    "# 4. Standardize Country Codes Across DataFrames\n",
    "def standardize_CountryCode(df, column='Country Code'):\n",
    "    df[column] = df[column].str.upper().str.strip()\n",
    "    return df\n",
    "\n",
    "medal_counts = standardize_CountryCode(medal_counts, 'Country Code')\n",
    "hosts = standardize_CountryCode(hosts, 'Country Code')        # Assuming hosts has 'Country Code'\n",
    "athlete_counts_agg = standardize_CountryCode(athlete_counts_agg, 'Country Code')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Aggregate the programs DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Programs DataFrame (Long Format):\n",
      "      Sport         Discipline Code Sports Governing Body  Year  Event_Count\n",
      "0  Aquatics  Artistic Swimming  SWA        World Aquatics  1896          0.0\n",
      "1  Aquatics             Diving  DIV        World Aquatics  1896          0.0\n",
      "2  Aquatics  Marathon Swimming  OWS        World Aquatics  1896          0.0\n",
      "3  Aquatics           Swimming  SWM        World Aquatics  1896          4.0\n",
      "4  Aquatics         Water Polo  WPO        World Aquatics  1896          0.0\n"
     ]
    }
   ],
   "source": [
    "# 2. Reshape 'programs' DataFrame from Wide to Long Format\n",
    "# Identify year columns (assuming they are all numeric)\n",
    "year_columns = [col for col in programs.columns if col.isdigit()]\n",
    "\n",
    "# Melt the DataFrame\n",
    "programs_long = programs.melt(\n",
    "    id_vars=['Sport', 'Discipline', 'Code', 'Sports Governing Body'],\n",
    "    value_vars=year_columns,\n",
    "    var_name='Year',\n",
    "    value_name='Event_Count'\n",
    ")\n",
    "\n",
    "# Convert 'Year' to integer\n",
    "programs_long['Year'] = programs_long['Year'].astype(int)\n",
    "\n",
    "# Preview the reshaped DataFrame\n",
    "print(\"Reshaped Programs DataFrame (Long Format):\")\n",
    "print(programs_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Event Data per Year:\n",
      "   Year  Total_Events  Number_of_Sports\n",
      "0  1896         107.0                51\n",
      "1  1900         236.0                51\n",
      "2  1904         224.0                51\n",
      "3  1906         176.0                51\n",
      "4  1908         267.0                51\n"
     ]
    }
   ],
   "source": [
    "# 3. Aggregate Event Counts and Number of Sports per Year\n",
    "event_agg = programs_long.groupby('Year').agg(\n",
    "    Total_Events=('Event_Count', 'sum'),\n",
    "    Number_of_Sports=('Sport', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Preview the aggregated data\n",
    "print(\"Aggregated Event Data per Year:\")\n",
    "print(event_agg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge All DataFrames into merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged Medal Counts with Athlete Counts successfully.\n"
     ]
    }
   ],
   "source": [
    "if not medal_counts.empty and not athlete_counts_agg.empty:\n",
    "    merged_df = pd.merge(\n",
    "        medal_counts,\n",
    "        athlete_counts_agg,\n",
    "        on=['Year', 'Country Code'],\n",
    "        how='left'\n",
    "    )\n",
    "    print(\"\\nMerged Medal Counts with Athlete Counts successfully.\")\n",
    "else:\n",
    "    merged_df = medal_counts.copy()\n",
    "    merged_df['Total_Athletes'] = 0\n",
    "    print(\"\\nAthlete Counts Aggregated DataFrame is empty. 'Total_Athletes' set to 0 in merged_df.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Total_Athletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Total_Athletes' missing values filled with 0.\n"
     ]
    }
   ],
   "source": [
    "if 'Total_Athletes' in merged_df.columns:\n",
    "    merged_df['Total_Athletes'] = merged_df['Total_Athletes'].fillna(0).astype(int)\n",
    "    print(\"'Total_Athletes' missing values filled with 0.\")\n",
    "else:\n",
    "    merged_df['Total_Athletes'] = 0\n",
    "    print(\"'Total_Athletes' column not found. Created and set to 0.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with hosts DataFrame to Set Is_Host Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Is_Host' indicator set successfully.\n"
     ]
    }
   ],
   "source": [
    "if not hosts.empty:\n",
    "    # Assuming 'hosts' DataFrame has 'Year' and 'Country Code' indicating the host country each year\n",
    "    host_info = set(hosts[['Year', 'Country Code']].drop_duplicates().itertuples(index=False, name=None))\n",
    "    \n",
    "    # Create 'Is_Host' column\n",
    "    merged_df['Is_Host'] = merged_df.apply(\n",
    "        lambda row: 1 if (row['Year'], row['Country Code']) in host_info else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    print(\"'Is_Host' indicator set successfully.\")\n",
    "else:\n",
    "    merged_df['Is_Host'] = 0\n",
    "    print(\"Hosts DataFrame is empty. 'Is_Host' set to 0.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Aggregated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged with Aggregated Event Data successfully.\n",
      "Unique countries: 152\n",
      "Total rows in merged_df: 1419\n",
      "Warning: There are duplicate Country Code entries. Aggregating data to ensure one row per country.\n",
      "Data aggregated to have one row per country.\n",
      "\n",
      "Final Merged DataFrame:\n",
      "  Country Code  Total  Total_Athletes  Is_Host  Number_of_Sports  Total_Events\n",
      "0          AFG      2              48        0               102          1343\n",
      "1          AHO      1              32        0                51           528\n",
      "2          ALB      2              26        0                51           738\n",
      "3          ALG     20             656        0               408          5099\n",
      "4          ANZ     12              40        0               102           503\n"
     ]
    }
   ],
   "source": [
    "if not event_agg.empty:\n",
    "    merged_df = pd.merge(\n",
    "        merged_df,\n",
    "        event_agg,\n",
    "        on='Year',\n",
    "        how='left'\n",
    "    )\n",
    "    # Fill missing values with 0\n",
    "    merged_df['Total_Events'] = merged_df['Total_Events'].fillna(0).astype(int)\n",
    "    merged_df['Number_of_Sports'] = merged_df['Number_of_Sports'].fillna(0).astype(int)\n",
    "    print(\"Merged with Aggregated Event Data successfully.\")\n",
    "else:\n",
    "    merged_df['Total_Events'] = 0\n",
    "    merged_df['Number_of_Sports'] = 0\n",
    "    print(\"Aggregated Event Data is empty. 'Total_Events' and 'Number_of_Sports' set to 0.\")\n",
    "\n",
    "\n",
    "# merged_df['Athletes_per_Event'] = merged_df['Total_Athletes'] / merged_df['Total_Events'].replace(0, 1)  # Avoid division by zero\n",
    "# Check for unique Country Code\n",
    "unique_countries = merged_df['Country Code'].nunique()\n",
    "total_rows = merged_df.shape[0]\n",
    "print(f\"Unique countries: {unique_countries}\")\n",
    "print(f\"Total rows in merged_df: {total_rows}\")\n",
    "\n",
    "if unique_countries != total_rows:\n",
    "    print(\"Warning: There are duplicate Country Code entries. Aggregating data to ensure one row per country.\")\n",
    "    # Aggregate data (e.g., sum of features) to have one row per country\n",
    "    merged_df = merged_df.groupby('Country Code').agg({\n",
    "        'Total': 'sum',\n",
    "        'Total_Athletes': 'sum',\n",
    "        'Is_Host': 'max',  # Assuming Is_Host is binary (0 or 1)\n",
    "        'Number_of_Sports': 'sum',\n",
    "        'Total_Events': 'sum',\n",
    "        # Add other relevant features as needed\n",
    "    }).reset_index()\n",
    "    print(\"Data aggregated to have one row per country.\")\n",
    "else:\n",
    "    print(\"All Country Code entries are unique.\")\n",
    "\n",
    "\n",
    "print(\"\\nFinal Merged DataFrame:\")\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables for Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependent variable set to 'Total'.\n"
     ]
    }
   ],
   "source": [
    "# Define the dependent variable\n",
    "dependent_var = 'Total'  # This is the total medal count\n",
    "\n",
    "# Verify if 'Total' exists in merged_df\n",
    "if dependent_var not in merged_df.columns:\n",
    "    print(f\"Error: Dependent variable '{dependent_var}' not found in merged_df.\")\n",
    "    # Optionally, inspect available columns\n",
    "    print(\"Available columns:\", merged_df.columns.tolist())\n",
    "else:\n",
    "    print(f\"\\nDependent variable set to '{dependent_var}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All independent variables are present in merged_df.\n"
     ]
    }
   ],
   "source": [
    "# Define the list of independent variables\n",
    "independent_vars = ['Total_Athletes', 'Is_Host', 'Number_of_Sports', 'Total_Events']\n",
    "\n",
    "# Verify if all independent variables exist in merged_df\n",
    "missing_vars = [var for var in independent_vars if var not in merged_df.columns]\n",
    "if missing_vars:\n",
    "    print(f\"Warning: The following independent variables are missing in merged_df: {missing_vars}\")\n",
    "    # Handle missing variables, e.g., create them with default values\n",
    "    for var in missing_vars:\n",
    "        merged_df[var] = 0\n",
    "    print(f\"Missing independent variables {missing_vars} created with default value 0.\")\n",
    "else:\n",
    "    print(\"All independent variables are present in merged_df.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged DataFrame saved successfully to /Users/chris/MCM_2025_C/Data/merged_data.csv.\n"
     ]
    }
   ],
   "source": [
    "# Define the output path\n",
    "output_path = '/Users/chris/MCM_2025_C/Data/merged_data.csv'\n",
    "\n",
    "# Save merged_df to CSV\n",
    "try:\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nMerged DataFrame saved successfully to {output_path}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving merged DataFrame to CSV: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in X:\n",
      "Total_Athletes      0\n",
      "Is_Host             0\n",
      "Number_of_Sports    0\n",
      "Total_Events        0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in y:\n",
      "0\n",
      "\n",
      "Data split into training and testing sets successfully.\n"
     ]
    }
   ],
   "source": [
    "X = merged_df[independent_vars]\n",
    "y = merged_df[dependent_var]\n",
    "\n",
    "# Check for missing values in X and y\n",
    "print(\"\\nMissing values in X:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in y:\")\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Handle missing values if any\n",
    "if X.isnull().values.any():\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"Missing values in X filled with median.\")\n",
    "\n",
    "if y.isnull().values.any():\n",
    "    y = y.fillna(0)\n",
    "    print(\"Missing values in y filled with 0.\")\n",
    "\n",
    "# Split the data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nData split into training and testing sets successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaling applied successfully.\n"
     ]
    }
   ],
   "source": [
    "# 10. Feature Scaling (If Needed)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling applied successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicitons and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Poisson Regression Model Summary:\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                  Total   No. Observations:                  121\n",
      "Model:                            GLM   Df Residuals:                      116\n",
      "Model Family:                 Poisson   Df Model:                            4\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -6759.5\n",
      "Date:                Mon, 27 Jan 2025   Deviance:                       12942.\n",
      "Time:                        16:47:05   Pearson chi2:                 1.96e+04\n",
      "No. Iterations:                     6   Pseudo R-squ. (CS):              1.000\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          4.0969      0.013    309.017      0.000       4.071       4.123\n",
      "x1             0.1694      0.008     21.080      0.000       0.154       0.185\n",
      "x2             0.5202      0.009     59.052      0.000       0.503       0.538\n",
      "x3             0.7663      0.059     12.906      0.000       0.650       0.883\n",
      "x4            -0.3684      0.059     -6.280      0.000      -0.483      -0.253\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # a. Poisson Regression\n",
    "# X_train_sm = sm.add_constant(X_train_scaled)\n",
    "# X_test_sm = sm.add_constant(X_test_scaled)\n",
    "\n",
    "# poisson_model = sm.GLM(y_train, X_train_sm, family=sm.families.Poisson()).fit()\n",
    "# print(\"\\nPoisson Regression Model Summary:\")\n",
    "# print(poisson_model.summary())\n",
    "\n",
    "# y_pred = poisson_model.predict(X_test_sm)\n",
    "\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "# print(f\"\\nPoisson Regression - MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# # b. Negative Binomial Regression\n",
    "# nb_model = sm.GLM(y_train, X_train_sm, family=sm.families.NegativeBinomial()).fit()\n",
    "# print(\"\\nNegative Binomial Regression Model Summary:\")\n",
    "# print(nb_model.summary())\n",
    "\n",
    "# y_pred_nb = nb_model.predict(X_test_sm)\n",
    "\n",
    "# mae_nb = mean_absolute_error(y_test, y_pred_nb)\n",
    "# rmse_nb = mean_squared_error(y_test, y_pred_nb, squared=False)\n",
    "# print(f\"\\nNegative Binomial Regression - MAE: {mae_nb:.2f}, RMSE: {rmse_nb:.2f}\")\n",
    "\n",
    "# # c. Random Forest Regressor\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "# print(\"\\nRandom Forest Regressor trained successfully.\")\n",
    "\n",
    "# y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "# rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
    "# r2_rf = r2_score(y_test, y_pred_rf)\n",
    "# print(f\"\\nRandom Forest Regressor - MAE: {mae_rf:.2f}, RMSE: {rmse_rf:.2f}, R²: {r2_rf:.2f}\")\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Add a constant for the intercept\n",
    "X_train_sm = sm.add_constant(X_train_scaled)\n",
    "X_test_sm = sm.add_constant(X_test_scaled)\n",
    "\n",
    "# Fit Poisson Regression\n",
    "poisson_model = sm.GLM(y_train, X_train_sm, family=sm.families.Poisson()).fit()\n",
    "print(\"\\nPoisson Regression Model Summary:\")\n",
    "print(poisson_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulations completed successfully.\n",
      "\n",
      "Number of unique training countries: 121\n",
      "Simulation DataFrame created successfully with correct column alignment.\n",
      "\n",
      "Prediction intervals calculated successfully.\n",
      "     Lower_95%  Median  Upper_95%\n",
      "CRC     24.000    35.0     46.025\n",
      "CAN    713.000   768.0    830.025\n",
      "GBR   1134.975  1204.5   1270.000\n",
      "KAZ     28.000    40.0     52.000\n",
      "BAR     21.000    32.0     43.000\n",
      "\n",
      "Aggregated y_train by Country_Code:\n",
      "Country Code\n",
      "AHO      5\n",
      "ALB      3\n",
      "ALG      4\n",
      "ANZ      1\n",
      "ARG    981\n",
      "Name: Total, dtype: int64\n",
      "\n",
      "Prediction Intervals after Sorting:\n",
      "     Lower_95%  Median  Upper_95%\n",
      "AHO       21.0    32.0     44.000\n",
      "ALB       21.0    31.0     43.000\n",
      "ALG       27.0    40.0     52.025\n",
      "ANZ       24.0    35.0     48.000\n",
      "ARG       74.0    92.0    111.025\n",
      "\n",
      "Final DataFrame with Prediction Intervals:\n",
      "  Country_Code  Total  Lower_95%  Median  Upper_95%\n",
      "0          AHO      5       21.0    32.0     44.000\n",
      "1          ALB      3       21.0    31.0     43.000\n",
      "2          ALG      4       27.0    40.0     52.025\n",
      "3          ANZ      1       24.0    35.0     48.000\n",
      "4          ARG    981       74.0    92.0    111.025\n",
      "\n",
      "Simulation results saved successfully to /Users/chris/MCM_2025_C/Datamedal_count_simulation_2028.csv.\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "num_simulations = 1000\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "simulation_results = []\n",
    "for sim in range(num_simulations):\n",
    "    expected_counts = poisson_model.predict(X_train_sm)\n",
    "    simulated_medals = np.random.poisson(expected_counts)\n",
    "    simulation_results.append(simulated_medals)\n",
    "\n",
    "print(\"\\nSimulations completed successfully.\")\n",
    "\n",
    "# ---------------------------- #\n",
    "# 17. Create Simulation DataFrame with Unique Country_Code Columns\n",
    "# ---------------------------- #\n",
    "# Extract unique Country_Code values from training set\n",
    "train_cc_unique = train_cc.unique()\n",
    "num_unique_train_countries = len(train_cc_unique)\n",
    "print(f\"\\nNumber of unique training countries: {num_unique_train_countries}\")\n",
    "\n",
    "# Check if each simulation has the correct number of columns\n",
    "if simulation_results and len(simulation_results[0]) == num_unique_train_countries:\n",
    "    simulation_df = pd.DataFrame(simulation_results, columns=train_cc_unique)\n",
    "    print(\"Simulation DataFrame created successfully with correct column alignment.\")\n",
    "else:\n",
    "    print(\"Error: Mismatch between number of simulation columns and unique training countries.\")\n",
    "    print(f\"Expected columns: {num_unique_train_countries}, Actual columns: {len(simulation_results[0]) if simulation_results else 0}\")\n",
    "    # Handle mismatch (e.g., adjust simulation_results or train_cc_unique)\n",
    "    # For now, exiting to prevent further errors\n",
    "    import sys\n",
    "    sys.exit(\"Exiting due to column mismatch.\")\n",
    "\n",
    "# ---------------------------- #\n",
    "# 18. Calculate Prediction Intervals\n",
    "# ---------------------------- #\n",
    "prediction_intervals = simulation_df.quantile([0.025, 0.5, 0.975]).T\n",
    "prediction_intervals.columns = ['Lower_95%', 'Median', 'Upper_95%']\n",
    "print(\"\\nPrediction intervals calculated successfully.\")\n",
    "print(prediction_intervals.head())\n",
    "\n",
    "# ---------------------------- #\n",
    "# 19. Aggregate y_train by Country_Code\n",
    "# ---------------------------- #\n",
    "y_train_grouped = y_train.groupby(train_cc).sum()\n",
    "print(\"\\nAggregated y_train by Country_Code:\")\n",
    "print(y_train_grouped.head())\n",
    "\n",
    "# ---------------------------- #\n",
    "# 20. Align Prediction Intervals with Aggregated Data\n",
    "# ---------------------------- #\n",
    "# Ensure that train_cc_unique matches y_train_grouped.index\n",
    "train_cc_unique_ordered = y_train_grouped.index.tolist()\n",
    "\n",
    "# Reindex prediction_intervals to match train_cc_unique_ordered\n",
    "prediction_intervals_sorted = prediction_intervals.reindex(train_cc_unique_ordered)\n",
    "\n",
    "# Check for any NaNs introduced by reindexing\n",
    "if prediction_intervals_sorted.isnull().values.any():\n",
    "    print(\"\\nWarning: Some prediction intervals could not be aligned with the aggregated Country_Code.\")\n",
    "    # Fill NaNs with 0 or appropriate default values\n",
    "    prediction_intervals_sorted = prediction_intervals_sorted.fillna(0)\n",
    "    print(\"Filled NaNs in prediction intervals with 0.\")\n",
    "\n",
    "print(\"\\nPrediction Intervals after Sorting:\")\n",
    "print(prediction_intervals_sorted.head())\n",
    "\n",
    "# ---------------------------- #\n",
    "# 21. Create Final DataFrame with Prediction Intervals\n",
    "# ---------------------------- #\n",
    "final_with_intervals = pd.DataFrame({\n",
    "    'Country_Code': train_cc_unique_ordered,\n",
    "    'Total': y_train_grouped.values,\n",
    "    'Lower_95%': prediction_intervals_sorted['Lower_95%'].values,\n",
    "    'Median': prediction_intervals_sorted['Median'].values,\n",
    "    'Upper_95%': prediction_intervals_sorted['Upper_95%'].values\n",
    "})\n",
    "print(\"\\nFinal DataFrame with Prediction Intervals:\")\n",
    "print(final_with_intervals.head())\n",
    "\n",
    "# ---------------------------- #\n",
    "# 22. Save Final DataFrame to CSV\n",
    "# ---------------------------- #\n",
    "simulation_output_path = '/Users/chris/MCM_2025_C/Datamedal_count_simulation_2028.csv'\n",
    "final_with_intervals.to_csv(simulation_output_path, index=False)\n",
    "print(f\"\\nSimulation results saved successfully to {simulation_output_path}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
